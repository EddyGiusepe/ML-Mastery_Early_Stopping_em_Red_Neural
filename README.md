# Early Stopping na Red Neural

Aqui aprenderemos que **interromper** o treinamento de uma ``rede neural`` antes de ajustar (overfit, superdimensionado) o conjunto de dados de 
treinamento pode reduzir o superajuste e melhorar a generalização de redes neurais profundas.

Um grande desafio no treinamento de redes neurais é quanto tempo para treiná-las.

Muito pouco treinamento significa que o modelo não se adequará ao treino e aos conjuntos de teste. Demasiado treinamento significa que o modelo superestimará 
o conjunto de dados de treinamento e terá um desempenho ruim no conjunto de teste.




Thanks God!
